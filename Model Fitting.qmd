---
title: "Model Fitting"
author: "Trevor Lynch"
format: html
editor: visual
---


```{r}
library(caret)
library(Metrics)
```

## Introduction

> The overall purpose of this Model Fitting page is to formulate 3 candidate models for each type, Logistic Regression, Classification Trees, and Random Forest models. The candidate models differ in the predictor variables that are included to estimate classification of an observation as being diagnosed with Diabetes or not. 

```{r}
# Split the data into a training and testing dataset
set.seed(10101)
training_index <- createDataPartition(diabetes_final$Diabetes_binary, p = 0.7, list = F)
training <- as.data.frame(diabetes_tib[training_index,])
testing <- as.data.frame(diabetes_tib[-training_index,])
```

Information on LogLoss as a metric for our binary-response models: 
> LogLoss is a metric used to evaluate the performance of classification models, particularly those predicting probabilities for binary outcomes. This metric differs from the metric, Accuracy, by penalizing incorrect predictions more heavily as the predictions become more confident. It quantifies the difference between the predicted probabilities and the actual binary outcomes, with a lower log loss indicating better model performance when comparing different models on the same dataset. 

## Logistic Regression Models
> The goal of logistic regression is to model the probability that a given input (or set of inputs) belongs to a particular class. The logistic function ensures that predictions are constrained between 0 and 1, which is intuitive for binary outcomes. For all of these reasons, logistic regression models are best applied to models that include a binary response variable.  

```{r}
# Specify the resampling scheme to be used in all models
train_ctrl <- trainControl(method = "cv", 
                           number = 3, 
                           classProbs = TRUE, 
                           summaryFunction = mnLogLoss,
                           verboseIter = T)

# No tuning parameters for the logistic regression models
# Model 1 includes all 11 predictor variables: 
logreg_model1 <- train(make.names(Diabetes_binary) ~ ., 
                       data = training, 
                       method = "glm",
                       trControl = train_ctrl, 
                       metric = "logLoss")

# Produce the probability of Diabetes Diagnosis or not with prediction. 
log_mod1_preds <- predict(logreg_model1, newdata = testing, type = "prob")

# Determine the logLoss for by comparing the actual diagnoses from the testing data set with the ones that were predicted.
log_mod1_logloss <- Metrics::logLoss(as.numeric(testing$Diabetes_binary), log_mod1_preds$Diabetes)

# Model 2 includes the predictor variables related to subjects' health conditions: 
logreg_model2 <- train(make.names(Diabetes_binary) ~ HighBP + HighChol + BMI + GenHlth + Sex + Age, 
                       data = training, 
                       method = "glm",
                       trControl = train_ctrl, 
                       metric = "logLoss")

# Produce the probability of Diabetes Diagnosis or not with prediction. 
log_mod2_preds <- predict(logreg_model2, newdata = testing, type = "prob")

# Determine the logLoss for by comparing the actual diagnoses from the testing data set with the ones that were predicted.
log_mod2_logloss <- Metrics::logLoss(as.numeric(testing$Diabetes_binary), log_mod2_preds$Diabetes)

# Model 3 includes predictor variables related to subjects' lifestyle, also including Sex and Age: 
logreg_model3 <- train(make.names(Diabetes_binary) ~ Smoker + PhysActivity + Fruits + Veggies + DiffWalk + Sex + Age, 
                       data = training, 
                       method = "glm",
                       trControl = train_ctrl, 
                       metric = "logLoss")

# Produce the probability of Diabetes Diagnosis or not with prediction. 
log_mod3_preds <- predict(logreg_model3, newdata = testing, type = "prob")

# Determine the logLoss for by comparing the actual diagnoses from the testing data set with the ones that were predicted.
log_mod3_logloss <- Metrics::logLoss(as.numeric(testing$Diabetes_binary), log_mod3_preds$Diabetes)

# Compile the logLoss results from the Logistic Regression models
logistic_results <- data.frame(Model = c("Logisitic Reg. Model 1", "Logisitic Reg. Model 2", "Logisitic Reg. Model 3"), logLoss = c(log_mod1_logloss, log_mod2_logloss, log_mod3_logloss))
logistic_results
```
**Model 3** produced the lowest logLoss value among the Logistic Regression models and is therefore the model of choice among these 3.  


## Classification Tree
> A classification tree model is a decision tree that is used for classifying data into categories based on the predictor variable values. The tree model is created by splitting the data into subsets based on the values of predictor variables that maximize the separation of classes. It is also notably useful because it does not assume a linear relationship between the feature variables and the response, so it can handle non-linear relationships between those. A classification tree model is appropriate to use in this context because we are trying to predict a binary classification of 'Diabetes' or 'No Diabetes' deagnoses for the Diabetes data.

```{r}
# There is one parameter, 'cp', that needs to be specified for each model 
# Model 1 includes all 11 predictor variables: 
ctree_model1 <- train(make.names(Diabetes_binary) ~ ., 
                      data = training, 
                      method = "rpart",
                      trControl = train_ctrl, 
                      metric = "logLoss",
                      tuneGrid = data.frame(cp = seq(0.00001, 0.01001, by = 0.0001)))

# Produce the probability of Diabetes Diagnosis or not with prediction. 
ctree_mod1_preds <- predict(ctree_model1, newdata = testing, type = "prob")

# Determine the logLoss for by comparing the actual diagnoses from the testing data set with the ones that were predicted.
ctree_mod1_logloss <- Metrics::logLoss(as.numeric(testing$Diabetes_binary), ctree_mod1_preds$Diabetes)

# Model 2 includes the predictor variables related to subjects' health conditions: 
ctree_model2 <- train(make.names(Diabetes_binary) ~ HighBP + HighChol + BMI + GenHlth + Sex + Age, 
                      data = training, 
                      method = "rpart",
                      trControl = train_ctrl, 
                      metric = "logLoss",
                      tuneGrid = data.frame(cp = seq(0.00001, 0.01001, by = 0.0001)))

# Produce the probability of Diabetes Diagnosis or not with prediction. 
ctree_mod2_preds <- predict(ctree_model2, newdata = testing, type = "prob")

# Determine the logLoss for by comparing the actual diagnoses from the testing data set with the ones that were predicted.
ctree_mod2_logloss <- Metrics::logLoss(as.numeric(testing$Diabetes_binary), ctree_mod2_preds$Diabetes)

# Model 3 includes predictor variables related to subjects' lifestyle, also including Sex and Age: 
ctree_model3 <- train(make.names(Diabetes_binary) ~ Smoker + PhysActivity + Fruits + Veggies + DiffWalk + Sex + Age, 
                      data = training, 
                      method = "rpart",
                      trControl = train_ctrl, 
                      metric = "logLoss",
                      tuneGrid = data.frame(cp = seq(0.00001, 0.01001, by = 0.0001)))

# Produce the probability of Diabetes Diagnosis or not with prediction. 
ctree_mod3_preds <- predict(ctree_model3, newdata = testing, type = "prob")

# Determine the logLoss for by comparing the actual diagnoses from the testing data set with the ones that were predicted.
ctree_mod3_logloss <- Metrics::logLoss(as.numeric(testing$Diabetes_binary), ctree_mod3_preds$Diabetes)

# Compile the logLoss results from the Classification Tree models
ctree_results <- data.frame(Model = c("Classification Tree Model 1", "Classification Tree Model 2", "Classification Tree Model 3"), logLoss = c(ctree_mod1_logloss, ctree_mod2_logloss, ctree_mod3_logloss))
ctree_results
```
**Model 3** produced the lowest logLoss value among the Classification Tree models and is therefore the model of choice among these 3.  


## Random Forest
> A Random Forest model is an ensemble method that incorporates multiple decision trees to improve the accuracy and robustness of predictions. The technique uses bootstrapping (splitting into random subsets of the data) and random selection of the predictor variables. The randomness involved with selection of predictor variables is done to reduce overfitting of the model. The final prediction for an observations classification is made by aggregating the predictions from all trees and taking a "majority vote" on what the classification of the observation is. It is appropriate to use for models with a binary response variable since it can be used to perform Classification, similar to the methods discussed above. The ensemble approach of the random forest models mitigate the overfitting, or lack of generalizability, that comes with single tree models.  

```{r}
# There is one parameter, 'mtry', that needs to be specified for each model 
# Model 1 includes all 11 predictor variables: 
rf_model1 <- train(make.names(Diabetes_binary) ~ ., 
                      data = training, 
                      method = "rf",
                      trControl = train_ctrl, 
                      metric = "logLoss",
                      tuneGrid = data.frame(mtry = 1:11))

# Produce the probability of Diabetes Diagnosis or not with prediction. 
rf_mod1_preds <- predict(rf_model1, newdata = testing, type = "prob")

# Determine the logLoss for by comparing the actual diagnoses from the testing data set with the ones that were predicted.
rf_mod1_logloss <- Metrics::logLoss(as.numeric(testing$Diabetes_binary), rf_mod1_preds$Diabetes)

# Model 2 includes the predictor variables related to subjects' health conditions: 
rf_model2 <- train(make.names(Diabetes_binary) ~ HighBP + HighChol + BMI + GenHlth + Sex + Age, 
                      data = training, 
                      method = "rf",
                      trControl = train_ctrl, 
                      metric = "logLoss",
                      tuneGrid = data.frame(mtry = 1:6))

# Produce the probability of Diabetes Diagnosis or not with prediction. 
rf_mod2_preds <- predict(rf_model2, newdata = testing, type = "prob")

# Determine the logLoss for by comparing the actual diagnoses from the testing data set with the ones that were predicted.
rf_mod2_logloss <- Metrics::logLoss(as.numeric(testing$Diabetes_binary), rf_mod2_preds$Diabetes)

# Model 3 includes predictor variables related to subjects' lifestyle, also including Sex and Age: 
rf_model3 <- train(make.names(Diabetes_binary) ~ Smoker + PhysActivity + Fruits + Veggies + DiffWalk + Sex + Age, 
                      data = training, 
                      method = "rf",
                      trControl = train_ctrl, 
                      metric = "logLoss",
                      tuneGrid = data.frame(mtry = 1:7))

# Produce the probability of Diabetes Diagnosis or not with prediction. 
rf_mod3_preds <- predict(rf_model3, newdata = testing, type = "prob")

# Determine the logLoss for by comparing the actual diagnoses from the testing data set with the ones that were predicted.
rf_mod3_logloss <- Metrics::logLoss(as.numeric(testing$Diabetes_binary), rf_mod3_preds$Diabetes)

# Compile the logLoss results from the Classification Tree models
rf_results <- data.frame(Model = c("Random Forest Model 1", "Random Forest Model 2", "Random Forest Model 3"), logLoss = c(rf_mod1_logloss, rf_mod2_logloss, rf_mod3_logloss))
rf_results
```
**Model __** produced the lowest logLoss value among the Random Forest models and is therefore the model of choice among these 3.  


## Final Model Selection
```{r}
final_results <- data.frame(Model = c("Best Logistic Regression Model", "Best Classification Tree Model", "Best Random Forest Model"), logLoss = c(log_mod3_logloss, ctree_mod3_logloss, rf_mod3_logloss))
```

The overall best model was found to be **Classification Tree - Model 3** after comparing the best Logistic Regression model, Classification Tree, and Random Forest model.
